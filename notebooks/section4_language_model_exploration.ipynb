{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66363c7-6673-4fa9-9604-fa6249bcb46f",
   "metadata": {},
   "source": [
    "<center><b><font size=6>Language Models exploration<b><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f364021-ae20-4352-8190-e6dd92128ffb",
   "metadata": {},
   "source": [
    "This notebook ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f721331e-6512-4d56-9326-7f5e4b13a7c9",
   "metadata": {},
   "source": [
    "Experiment language models for solving the same supervised task as in Section 2. In this task, the objective\n",
    "is to harness the capabilities of language models like Bert or Word2Vec, for supervised learning (assign\n",
    "intents to sessions).\n",
    "Two interesting concepts play a role when we use neural networks:\n",
    "1- it is possible to do transfer learning, i.e., to take a model that have been trained with other\n",
    "enormous datasets by Big Tech companies, and we can do fine-tuning i.e., to train this model\n",
    "starting from its pre-trained version.\n",
    "2- In NLP tasks, words/documents are transformed into vectors (encoding) and this task is\n",
    "Unsupervised, so we can use a much larger amount of data.\n",
    " Choose a language model between Bert and Doc2Vec (word2vec for documents), then:\n",
    "1. If you choose Doc2Vec: pretrain Doc2Vec on body column of the session text. If you chose Bert: take the pretrained Bert model like in this example. (NB: In this tutorial they used BertForSequenceClassification, but if you want to continue with step 2, you must take an other Bert implementation from HuggingFace)\n",
    "2. Add a last Dense Layer\n",
    "3. Fine-tune the last layer of the network on the supervised training set for N epochs.\n",
    "4. Plot the learning curves on training and validation set. After how many epochs should we stop the training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664039a1-74d6-46f0-9e53-b1757ed81ec2",
   "metadata": {},
   "source": [
    "0. **Install Dependencies**\n",
    "1. ** ... **\n",
    "2. ** ... **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e94f7d-e1d3-424a-9b48-3b0af814105a",
   "metadata": {},
   "source": [
    "<center><b><font size=5>Install Dependencies<b><center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7681f7f5-8b34-4958-8ded-1e3088118b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages for section4: transformers\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mmm\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2024.4.16-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (761 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.6/761.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.3/436.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2022.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.11)\n",
      "Installing collected packages: tokenizers, safetensors, regex, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.12.2 huggingface-hub-0.16.4 regex-2024.4.16 safetensors-0.4.5 tokenizers-0.13.3 transformers-4.30.2\n",
      "Successfully installed: transformers\n"
     ]
    }
   ],
   "source": [
    "!python ../scripts/install_dependencies.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c82c91-acd6-417b-b0b6-9d9c37d0c080",
   "metadata": {},
   "source": [
    "<center><b><font size=5>Name<b><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9a879-fc92-49e2-8889-5b90417ef77b",
   "metadata": {},
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a4a09b-1937-4b12-8a41-991c65658c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyArrow\n",
      "  Using cached pyarrow-12.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.1 MB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.7/site-packages (from PyArrow) (1.21.6)\n",
      "Installing collected packages: PyArrow\n",
      "Successfully installed PyArrow-12.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyArrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18cad2ba-9384-4954-914d-19108cbd3d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.4.0)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.8.0)\n",
      "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d5ed4e-de88-41f6-8f82-9564b3e02fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2024.4.16-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (761 kB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Using cached safetensors-0.4.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2022.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.11)\n",
      "Installing collected packages: tokenizers, safetensors, regex, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.12.2 huggingface-hub-0.16.4 regex-2024.4.16 safetensors-0.4.5 tokenizers-0.13.3 transformers-4.30.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e99d38a2-55bc-42db-b748-f7085f176f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa26a980-3093-4cc1-9c9a-3ffb56e0a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load Dataset\n",
    "print(\"Loading the dataset...\")\n",
    "df = pd.read_csv(\"ssh_attacks_dataset.csv\")\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset size: {df.shape[0]} rows\")\n",
    "\n",
    "# 2. Preprocess Set_Fingerprint column (multi-label encoding)\n",
    "print(\"Preprocessing 'Set_Fingerprint' column...\")\n",
    "df['Set_Fingerprint'] = df['Set_Fingerprint'].apply(\n",
    "    lambda x: [intent.strip() for intent in x.split(',')]\n",
    ")\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(df['Set_Fingerprint'])\n",
    "print(f\"Classes identified: {mlb.classes_}\")\n",
    "\n",
    "# 3. Train-test split\n",
    "print(\"Splitting the data into training and validation sets...\")\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['full_session'], y, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"Data split complete.\")\n",
    "\n",
    "# 4. Tokenization\n",
    "print(\"Tokenizing the text data...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_texts = train_texts.fillna(\"\").astype(str)\n",
    "val_texts = val_texts.fillna(\"\").astype(str)\n",
    "\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=128)\n",
    "print(\"Tokenization complete.\")\n",
    "\n",
    "# 5. Create Custom Dataset Class\n",
    "print(\"Creating the custom dataset class...\")\n",
    "class SSHDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encodings['input_ids'][idx]),\n",
    "            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx]),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "print(\"Custom dataset class created.\")\n",
    "\n",
    "# 6. Prepare DataLoader\n",
    "print(\"Creating DataLoaders for training and validation...\")\n",
    "train_dataset = SSHDataset(train_encodings, train_labels)\n",
    "val_dataset = SSHDataset(val_encodings, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "print(\"DataLoaders are ready.\")\n",
    "\n",
    "# 7. Initialize the Model\n",
    "print(\"Initializing the BERT model for sequence classification...\")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=y.shape[1])\n",
    "model.to(device)\n",
    "print(\"Model initialized and moved to device:\", device)\n",
    "\n",
    "# 8. Optimizer and Loss\n",
    "print(\"Setting up optimizer and loss function...\")\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "print(\"Optimizer and loss function are ready.\")\n",
    "\n",
    "# 9. Training Loop\n",
    "print(\"Starting the training process...\")\n",
    "train_loss_list, val_loss_list = [], []\n",
    "\n",
    "for epoch in range(5):  # Fine-tune for 5 epochs\n",
    "    print(f\"Epoch {epoch+1} / 5\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = (\n",
    "            batch['input_ids'].to(device),\n",
    "            batch['attention_mask'].to(device),\n",
    "            batch['labels'].to(device),\n",
    "        )\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    train_loss_list.append(total_loss / len(train_loader))\n",
    "    print(f\"Training loss: {train_loss_list[-1]:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids, attention_mask, labels = (\n",
    "                batch['input_ids'].to(device),\n",
    "                batch['attention_mask'].to(device),\n",
    "                batch['labels'].to(device),\n",
    "            )\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            val_loss += loss.item()\n",
    "    val_loss_list.append(val_loss / len(val_loader))\n",
    "    print(f\"Validation loss: {val_loss_list[-1]:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# 10. Plot Learning Curves\n",
    "print(\"Plotting the learning curves...\")\n",
    "plt.plot(range(1, 6), train_loss_list, label=\"Training Loss\")\n",
    "plt.plot(range(1, 6), val_loss_list, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"Learning curves plotted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62be27cf-9f10-4692-9f5f-368a8396ec6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
