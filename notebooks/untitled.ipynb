{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92c9e7c-3fc3-43ff-a899-59b42b961e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import logging   # For logging messages and debugging information\n",
    "import datetime  # For tracking execution time and timestamps\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-party imports for data processing and machine learning\n",
    "import torch                                           # PyTorch deep learning framework\n",
    "from torch import nn                                   # Neural network modules\n",
    "from torch.utils.data import Dataset, DataLoader       # Data handling utilities\n",
    "import pandas as pd                                    # For DataFrame operations\n",
    "import numpy as np                                     # For numerical operations\n",
    "from sklearn.model_selection import train_test_split   # For splitting dataset\n",
    "from sklearn.preprocessing import MultiLabelBinarizer  # For label encoding\n",
    "import matplotlib.pyplot as plt                        # For plotting learning curves\n",
    "from tqdm import tqdm                                  # For progress bars\n",
    "\n",
    "# Hugging Face transformers imports\n",
    "from transformers import BertModel, BertTokenizer, AdamW  # BERT model and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c58a085c-aa34-47d9-b0a0-be2619ddb9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 09:17:09 - INFO - Starting program and initializing imports...\n"
     ]
    }
   ],
   "source": [
    "# Configure logging to show timestamp, log level, and message\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Show all info messages and above (info, warning, error, critical)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Format: timestamp - level - message\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'  # Date format for the timestamp\n",
    ")\n",
    "\n",
    "# Log the start of the program and verify logging is working\n",
    "logging.info(\"Starting program and initializing imports...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d8997b7-b03a-47c3-98c3-c7127e8c6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShellAttackDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Log dataset statistics\n",
    "        logging.info(f\"Created dataset with {len(texts)} samples\")\n",
    "        logging.info(f\"Number of labels: {labels.shape[1]}\")\n",
    "        \n",
    "        # Calculate and log average sequence length\n",
    "        avg_len = np.mean([len(\" \".join(text).split()) for text in texts])\n",
    "        logging.info(f\"Average sequence length (words): {avg_len:.2f}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = \" \".join(self.texts[idx])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.FloatTensor(self.labels[idx])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416dad3c-8ee7-4143-bef7-d41763c189fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Load pre-trained BERT\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Add custom classification head\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Log model architecture details\n",
    "        logging.info(f\"Initialized BERT Classifier with:\")\n",
    "        logging.info(f\"- BERT hidden size: {self.bert.config.hidden_size}\")\n",
    "        logging.info(f\"- Number of labels: {num_labels}\")\n",
    "        logging.info(f\"- Dropout rate: 0.1\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use the [CLS] token representation\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Apply dropout and classification\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.classifier(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eace7ff-2c3b-4e72-99cd-18102320691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, num_epochs=10):\n",
    "    # Initialize optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    # Binary Cross Entropy loss for multi-label classification\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Calculate total number of training steps\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    logging.info(f\"Starting training with {total_steps} total steps\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_steps = 0\n",
    "        \n",
    "        epoch_start_time = datetime.now()\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "        \n",
    "        avg_train_loss = total_train_loss / train_steps\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_steps = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                val_steps += 1\n",
    "        \n",
    "        avg_val_loss = total_val_loss / val_steps\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        epoch_time = datetime.now() - epoch_start_time\n",
    "        \n",
    "        logging.info(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        logging.info(f\"Time taken: {epoch_time}\")\n",
    "        logging.info(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "        logging.info(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Calculate and log loss improvement\n",
    "        if epoch > 0:\n",
    "            train_improvement = train_losses[-2] - train_losses[-1]\n",
    "            val_improvement = val_losses[-2] - val_losses[-1]\n",
    "            logging.info(f\"Training loss improvement: {train_improvement:.4f}\")\n",
    "            logging.info(f\"Validation loss improvement: {val_improvement:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c445c87-2f06-46d3-944b-1a5455a63092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_results(model, training_history, mlb, save_path='training_results.pt'):\n",
    "    \"\"\"Save model, training history and label binarizer.\"\"\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'training_history': training_history,\n",
    "        'label_binarizer_classes': mlb.classes_\n",
    "    }, save_path)\n",
    "    logging.info(f\"Training results saved to {save_path}\")\n",
    "\n",
    "def load_training_results(model_class, save_path='training_results.pt'):\n",
    "    \"\"\"Load model and training history.\"\"\"\n",
    "    checkpoint = torch.load(save_path)\n",
    "    \n",
    "    # Recreate model with same number of labels\n",
    "    model = model_class(num_labels=len(checkpoint['label_binarizer_classes']))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    return model, checkpoint['training_history'], checkpoint['label_binarizer_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16cf243c-4081-445b-b4f1-3e2806ac5c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 09:17:09 - INFO - Starting data processing and model training pipeline\n",
      "2025-01-10 09:17:09 - INFO - Loading dataset from parquet file...\n",
      "2025-01-10 09:17:11 - INFO - Loaded dataset with 233035 rows\n",
      "2025-01-10 09:17:11 - INFO - Sampled dataset with 11652 rows (percentage=5.0%)\n",
      "2025-01-10 09:17:11 - INFO - \n",
      "Dataset Statistics:\n",
      "2025-01-10 09:17:11 - INFO - Number of unique session IDs: 233035\n",
      "2025-01-10 09:17:11 - INFO - Date range: 2019-06-04 09:45:11.151186+00:00 to 2020-02-29 23:59:22.199490+00:00\n"
     ]
    }
   ],
   "source": [
    "# Start timing\n",
    "start_time = datetime.now()\n",
    "logging.info(\"Starting data processing and model training pipeline\")\n",
    "    \n",
    "# Load data\n",
    "logging.info(\"Loading dataset from parquet file...\")\n",
    "df = pd.read_parquet(\"../data/processed/ssh_attacks_decoded.parquet\")\n",
    "logging.info(f\"Loaded dataset with {len(df)} rows\")\n",
    "\n",
    "\"\"\"\n",
    "Temporarily working on only a percentage of the dataset\n",
    "\"\"\"\n",
    "\n",
    "# Define percentage of the dataset to work on\n",
    "percentage = 0.05  # 5%\n",
    "\n",
    "# Sample the dataset\n",
    "df_sampled = df.sample(frac=percentage, random_state=42)\n",
    "logging.info(f\"Sampled dataset with {len(df_sampled)} rows (percentage={percentage*100}%)\")\n",
    "    \n",
    "# Log data statistics\n",
    "logging.info(\"\\nDataset Statistics:\")\n",
    "logging.info(f\"Number of unique session IDs: {df['session_id'].nunique()}\")\n",
    "logging.info(f\"Date range: {df['first_timestamp'].min()} to {df['first_timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02ab76df-12dd-445e-b10c-ecaa2116f26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 09:17:16 - INFO - \n",
      "Processing labels...\n",
      "2025-01-10 09:17:17 - INFO - Number of unique labels: 7\n",
      "2025-01-10 09:17:17 - INFO - Most common labels:\n",
      "2025-01-10 09:17:17 - INFO - - Discovery: 232145 occurrences\n",
      "2025-01-10 09:17:17 - INFO - - Persistence: 211295 occurrences\n",
      "2025-01-10 09:17:17 - INFO - - Execution: 92927 occurrences\n",
      "2025-01-10 09:17:17 - INFO - - Defense Evasion: 18999 occurrences\n",
      "2025-01-10 09:17:17 - INFO - - Harmless: 2206 occurrences\n"
     ]
    }
   ],
   "source": [
    "# Process labels\n",
    "logging.info(\"\\nProcessing labels...\")\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform([set(x) for x in df['Set_Fingerprint']])\n",
    "logging.info(f\"Number of unique labels: {len(mlb.classes_)}\")\n",
    "logging.info(\"Most common labels:\")\n",
    "label_counts = pd.Series([label for labels_list in df['Set_Fingerprint'] for label in labels_list]).value_counts()\n",
    "for label, count in label_counts.head().items():\n",
    "    logging.info(f\"- {label}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5af158a-98b7-4d35-ba78-8acfb2876c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 09:17:17 - INFO - \n",
      "Splitting data into train and validation sets...\n",
      "2025-01-10 09:17:17 - INFO - Training set size: 186428\n",
      "2025-01-10 09:17:17 - INFO - Validation set size: 46607\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "logging.info(\"\\nSplitting data into train and validation sets...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df['full_session'].values,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "logging.info(f\"Training set size: {len(X_train)}\")\n",
    "logging.info(f\"Validation set size: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3ebbe59-0112-4d09-a533-814678b44f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 09:17:18 - INFO - \n",
      "Initializing BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "logging.info(\"\\nInitializing BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34c9ad61-5400-44f4-8ecd-08af0d986719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 09:17:18 - INFO - Creating datasets...\n",
      "2025-01-10 09:17:18 - INFO - Created dataset with 186428 samples\n",
      "2025-01-10 09:17:18 - INFO - Number of labels: 7\n",
      "2025-01-10 09:17:19 - INFO - Average sequence length (words): 69.70\n",
      "2025-01-10 09:17:19 - INFO - Created dataset with 46607 samples\n",
      "2025-01-10 09:17:19 - INFO - Number of labels: 7\n",
      "2025-01-10 09:17:20 - INFO - Average sequence length (words): 69.59\n",
      "2025-01-10 09:17:20 - INFO - \n",
      "Creating data loaders with batch size 16...\n",
      "2025-01-10 09:17:20 - INFO - Number of training batches: 11652\n",
      "2025-01-10 09:17:20 - INFO - Number of validation batches: 2913\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "logging.info(\"Creating datasets...\")\n",
    "train_dataset = ShellAttackDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = ShellAttackDataset(X_val, y_val, tokenizer)\n",
    "    \n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "logging.info(f\"\\nCreating data loaders with batch size {batch_size}...\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "logging.info(f\"Number of training batches: {len(train_loader)}\")\n",
    "logging.info(f\"Number of validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4048a07e-558a-4a3d-a19c-70c5c0437229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 09:17:20 - INFO - \n",
      "Initializing model...\n",
      "2025-01-10 09:17:20 - INFO - Using device: cpu\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-01-10 09:17:26 - INFO - Initialized BERT Classifier with:\n",
      "2025-01-10 09:17:26 - INFO - - BERT hidden size: 768\n",
      "2025-01-10 09:17:26 - INFO - - Number of labels: 7\n",
      "2025-01-10 09:17:26 - INFO - - Dropout rate: 0.1\n",
      "2025-01-10 09:17:26 - INFO - Total parameters: 109,487,623\n",
      "2025-01-10 09:17:26 - INFO - Trainable parameters: 109,487,623\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "logging.info(\"\\nInitializing model...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging.info(f\"Using device: {device}\")\n",
    "model = BertClassifier(num_labels=len(mlb.classes_))\n",
    "model.to(device)\n",
    "    \n",
    "# Count model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logging.info(f\"Total parameters: {total_params:,}\")\n",
    "logging.info(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64600a25-6d2b-4bbb-8b7f-64a480d2e5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 09:17:33 - INFO - \n",
      "Starting model training...\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "2025-01-10 09:17:33 - INFO - Starting training with 46608 total steps\n",
      "Epoch 1/4:   0%|          | 1/11652 [01:31<296:22:43, 91.58s/it]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "logging.info(\"\\nStarting model training...\")\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader, device, num_epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f3a38-8705-4bf5-9bf9-cc019080b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training:\n",
    "training_history = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'true_labels': y_true,\n",
    "    'predicted_probabilities': y_pred_probs,\n",
    "    'probabilities_per_epoch': y_pred_probs_per_epoch\n",
    "}\n",
    "\n",
    "save_training_results(model, training_history, mlb, save_path='../results/models/training_results.pt')\n",
    "\n",
    "# Later, to load:\n",
    "# model, history, class_names = load_training_results(BertClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee8c52-0126-4e3c-bc08-6380711dde0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc7075-92fb-4c85-bf76-a4ba7b3e41d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "logging.info(\"Plotting learning curves...\")\n",
    "plot_learning_curves(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ddfdd-c5f3-4c62-9264-3061832d8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log total execution time\n",
    "total_time = datetime.now() - start_time\n",
    "logging.info(f\"\\nTotal execution time: {total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4b95e-3f29-4c8a-9a00-2250c2e7012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31157995-edf0-4aea-bdcd-6efc23007a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "global_overwrite = True\n",
    "plot_directory = \"../results/figures/plots/section4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ec6452-00ac-4377-a774-e75a62b3ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.data_storage_utils import save_plot, plot_and_save\n",
    "from scripts.plotting_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f2874-99c7-4273-9a3c-9c4ea725033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have these variables from your model training:\n",
    "# y_true: Ground truth labels\n",
    "# y_pred_probs: Model's predicted probabilities\n",
    "# train_losses: List of training losses per epoch\n",
    "# val_losses: List of validation losses per epoch\n",
    "# class_names: List of your label names\n",
    "# y_pred_probs_per_epoch: List of prediction probabilities for each epoch\n",
    "\n",
    "y_true = model_evaluation['true_labels']\n",
    "y_pred_probs = model_evaluation['predicted_probabilities']\n",
    "y_pred_probs_per_epoch = model_evaluation['probabilities_per_epoch']\n",
    "train_losses = model_evaluation['train_losses']\n",
    "val_losses = model_evaluation['val_losses']\n",
    "class_names = mlb.classes_  # from your MultiLabelBinarizer\n",
    "\n",
    "# Generate all plots\n",
    "\n",
    "# 1. Metrics over epochs\n",
    "plot_metrics_over_epochs(y_true, y_pred_probs_per_epoch, len(train_losses))\n",
    "    \n",
    "# 2. Loss curves\n",
    "plot_loss_curves(train_losses, val_losses)\n",
    "    \n",
    "# 3. ROC curves\n",
    "plot_roc_curves(y_true, y_pred_probs, class_names)\n",
    "    \n",
    "# 4. Precision-Recall curves\n",
    "plot_pr_curves(y_true, y_pred_probs, class_names)\n",
    "    \n",
    "# 5. Probability histograms\n",
    "plot_prob_histograms(y_pred_probs, class_names)\n",
    "    \n",
    "# 6. 3D ROC curve\n",
    "plot_3d_roc(y_true, y_pred_probs, class_names)\n",
    "    \n",
    "# 7. F1 scores\n",
    "plot_f1_scores(y_true, y_pred_probs, class_names)\n",
    "    \n",
    "# 8. Performance metrics\n",
    "plot_performance_metrics(y_true, y_pred_probs, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc0532-27b7-4318-a144-33330ed9447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and save all plots using plot_and_save\n",
    "plots_to_generate = [\n",
    "    {\n",
    "        \"func\": plot_metrics_over_epochs,\n",
    "        \"args\": {\"y_true\": y_true, \"y_pred_probs_per_epoch\": y_pred_probs_per_epoch, \"num_epochs\": len(train_losses)},\n",
    "        \"filename\": \"metrics_over_epochs\"\n",
    "    },\n",
    "    {\n",
    "        \"func\": plot_loss_curves,\n",
    "        \"args\": {\"train_losses\": train_losses, \"val_losses\": val_losses},\n",
    "        \"filename\": \"loss_curves\"\n",
    "    },\n",
    "    {\n",
    "        \"func\": plot_roc_curves,\n",
    "        \"args\": {\"y_true\": y_true, \"y_pred_probs\": y_pred_probs, \"class_names\": class_names},\n",
    "        \"filename\": \"roc_curves\"\n",
    "    },\n",
    "    {\n",
    "        \"func\": plot_pr_curves,\n",
    "        \"args\": {\"y_true\": y_true, \"y_pred_probs\": y_pred_probs, \"class_names\": class_names},\n",
    "        \"filename\": \"precision_recall_curves\"\n",
    "    },\n",
    "    {\n",
    "        \"func\": plot_prob_histograms,\n",
    "        \"args\": {\"y_pred_probs\": y_pred_probs, \"class_names\": class_names},\n",
    "        \"filename\": \"probability_histograms\"\n",
    "    },\n",
    "    {\n",
    "        \"func\": plot_3d_roc,\n",
    "        \"args\": {\"y_true\": y_true, \"y_pred_probs\": y_pred_probs, \"class_names\": class_names},\n",
    "        \"filename\": \"3d_roc_curve\"\n",
    "    },\n",
    "    {\n",
    "        \"func\": plot_f1_scores,\n",
    "        \"args\": {\"y_true\": y_true, \"y_pred_probs\": y_pred_probs, \"class_names\": class_names},\n",
    "        \"filename\": \"f1_scores\"\n",
    "    },\n",
    "    {\n",
    "        \"func\": plot_performance_metrics,\n",
    "        \"args\": {\"y_true\": y_true, \"y_pred_probs\": y_pred_probs, \"class_names\": class_names},\n",
    "        \"filename\": \"performance_metrics\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Generate and save each plot\n",
    "for plot in plots_to_generate:\n",
    "    plot_and_save(\n",
    "        plot_func=plot[\"func\"],\n",
    "        plot_args=plot[\"args\"],\n",
    "        directory=plot_directory,\n",
    "        filename=plot[\"filename\"],\n",
    "        filetype=\"png\",\n",
    "        overwrite=global_overwrite,\n",
    "        show_plot=True  # Set to False if you don't want to display plots in the notebook\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb5e982-ebf9-4be0-ab74-36f5367d2686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
