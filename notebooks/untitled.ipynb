{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92c9e7c-3fc3-43ff-a899-59b42b961e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import logging   # For logging messages and debugging information\n",
    "import datetime  # For tracking execution time and timestamps\n",
    "from datetime import datetime\n",
    "\n",
    "# Third-party imports for data processing and machine learning\n",
    "import torch                                           # PyTorch deep learning framework\n",
    "from torch import nn                                   # Neural network modules\n",
    "from torch.utils.data import Dataset, DataLoader       # Data handling utilities\n",
    "import pandas as pd                                    # For DataFrame operations\n",
    "import numpy as np                                     # For numerical operations\n",
    "from sklearn.model_selection import train_test_split   # For splitting dataset\n",
    "from sklearn.preprocessing import MultiLabelBinarizer  # For label encoding\n",
    "import matplotlib.pyplot as plt                        # For plotting learning curves\n",
    "from tqdm import tqdm                                  # For progress bars\n",
    "\n",
    "# Hugging Face transformers imports\n",
    "from transformers import BertModel, BertTokenizer, AdamW  # BERT model and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c58a085c-aa34-47d9-b0a0-be2619ddb9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 08:34:00 - INFO - Starting program and initializing imports...\n"
     ]
    }
   ],
   "source": [
    "# Configure logging to show timestamp, log level, and message\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Show all info messages and above (info, warning, error, critical)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Format: timestamp - level - message\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'  # Date format for the timestamp\n",
    ")\n",
    "\n",
    "# Log the start of the program and verify logging is working\n",
    "logging.info(\"Starting program and initializing imports...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d8997b7-b03a-47c3-98c3-c7127e8c6bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShellAttackDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Log dataset statistics\n",
    "        logging.info(f\"Created dataset with {len(texts)} samples\")\n",
    "        logging.info(f\"Number of labels: {labels.shape[1]}\")\n",
    "        \n",
    "        # Calculate and log average sequence length\n",
    "        avg_len = np.mean([len(\" \".join(text).split()) for text in texts])\n",
    "        logging.info(f\"Average sequence length (words): {avg_len:.2f}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = \" \".join(self.texts[idx])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.FloatTensor(self.labels[idx])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "416dad3c-8ee7-4143-bef7-d41763c189fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Load pre-trained BERT\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Add custom classification head\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Log model architecture details\n",
    "        logging.info(f\"Initialized BERT Classifier with:\")\n",
    "        logging.info(f\"- BERT hidden size: {self.bert.config.hidden_size}\")\n",
    "        logging.info(f\"- Number of labels: {num_labels}\")\n",
    "        logging.info(f\"- Dropout rate: 0.1\")\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use the [CLS] token representation\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Apply dropout and classification\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.classifier(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eace7ff-2c3b-4e72-99cd-18102320691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, num_epochs=10):\n",
    "    # Initialize optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    # Binary Cross Entropy loss for multi-label classification\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Calculate total number of training steps\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    logging.info(f\"Starting training with {total_steps} total steps\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_steps = 0\n",
    "        \n",
    "        epoch_start_time = datetime.now()\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "        \n",
    "        avg_train_loss = total_train_loss / train_steps\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_steps = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                val_steps += 1\n",
    "        \n",
    "        avg_val_loss = total_val_loss / val_steps\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        epoch_time = datetime.now() - epoch_start_time\n",
    "        \n",
    "        logging.info(f\"\\nEpoch {epoch + 1} Summary:\")\n",
    "        logging.info(f\"Time taken: {epoch_time}\")\n",
    "        logging.info(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "        logging.info(f\"Average validation loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Calculate and log loss improvement\n",
    "        if epoch > 0:\n",
    "            train_improvement = train_losses[-2] - train_losses[-1]\n",
    "            val_improvement = val_losses[-2] - val_losses[-1]\n",
    "            logging.info(f\"Training loss improvement: {train_improvement:.4f}\")\n",
    "            logging.info(f\"Validation loss improvement: {val_improvement:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c445c87-2f06-46d3-944b-1a5455a63092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_results(model, training_history, mlb, save_path='training_results.pt'):\n",
    "    \"\"\"Save model, training history and label binarizer.\"\"\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'training_history': training_history,\n",
    "        'label_binarizer_classes': mlb.classes_\n",
    "    }, save_path)\n",
    "    logging.info(f\"Training results saved to {save_path}\")\n",
    "\n",
    "def load_training_results(model_class, save_path='training_results.pt'):\n",
    "    \"\"\"Load model and training history.\"\"\"\n",
    "    checkpoint = torch.load(save_path)\n",
    "    \n",
    "    # Recreate model with same number of labels\n",
    "    model = model_class(num_labels=len(checkpoint['label_binarizer_classes']))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    return model, checkpoint['training_history'], checkpoint['label_binarizer_classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16cf243c-4081-445b-b4f1-3e2806ac5c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 08:34:25 - INFO - Starting data processing and model training pipeline\n",
      "2025-01-10 08:34:25 - INFO - Loading dataset from parquet file...\n",
      "2025-01-10 08:34:27 - INFO - Loaded dataset with 233035 rows\n",
      "2025-01-10 08:34:27 - INFO - \n",
      "Dataset Statistics:\n",
      "2025-01-10 08:34:27 - INFO - Number of unique session IDs: 233035\n",
      "2025-01-10 08:34:27 - INFO - Date range: 2019-06-04 09:45:11.151186+00:00 to 2020-02-29 23:59:22.199490+00:00\n"
     ]
    }
   ],
   "source": [
    "# Start timing\n",
    "start_time = datetime.now()\n",
    "logging.info(\"Starting data processing and model training pipeline\")\n",
    "    \n",
    "# Load data\n",
    "logging.info(\"Loading dataset from parquet file...\")\n",
    "df = pd.read_parquet(\"../data/processed/ssh_attacks_decoded.parquet\")\n",
    "logging.info(f\"Loaded dataset with {len(df)} rows\")\n",
    "    \n",
    "# Log data statistics\n",
    "logging.info(\"\\nDataset Statistics:\")\n",
    "logging.info(f\"Number of unique session IDs: {df['session_id'].nunique()}\")\n",
    "logging.info(f\"Date range: {df['first_timestamp'].min()} to {df['first_timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02ab76df-12dd-445e-b10c-ecaa2116f26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 08:34:31 - INFO - \n",
      "Processing labels...\n",
      "2025-01-10 08:34:32 - INFO - Number of unique labels: 7\n",
      "2025-01-10 08:34:32 - INFO - Most common labels:\n",
      "2025-01-10 08:34:32 - INFO - - Discovery: 232145 occurrences\n",
      "2025-01-10 08:34:32 - INFO - - Persistence: 211295 occurrences\n",
      "2025-01-10 08:34:32 - INFO - - Execution: 92927 occurrences\n",
      "2025-01-10 08:34:32 - INFO - - Defense Evasion: 18999 occurrences\n",
      "2025-01-10 08:34:32 - INFO - - Harmless: 2206 occurrences\n"
     ]
    }
   ],
   "source": [
    "# Process labels\n",
    "logging.info(\"\\nProcessing labels...\")\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform([set(x) for x in df['Set_Fingerprint']])\n",
    "logging.info(f\"Number of unique labels: {len(mlb.classes_)}\")\n",
    "logging.info(\"Most common labels:\")\n",
    "label_counts = pd.Series([label for labels_list in df['Set_Fingerprint'] for label in labels_list]).value_counts()\n",
    "for label, count in label_counts.head().items():\n",
    "    logging.info(f\"- {label}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5af158a-98b7-4d35-ba78-8acfb2876c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 08:34:34 - INFO - \n",
      "Splitting data into train and validation sets...\n",
      "2025-01-10 08:34:34 - INFO - Training set size: 186428\n",
      "2025-01-10 08:34:34 - INFO - Validation set size: 46607\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "logging.info(\"\\nSplitting data into train and validation sets...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df['full_session'].values,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "logging.info(f\"Training set size: {len(X_train)}\")\n",
    "logging.info(f\"Validation set size: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3ebbe59-0112-4d09-a533-814678b44f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 08:34:35 - INFO - \n",
      "Initializing BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "logging.info(\"\\nInitializing BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34c9ad61-5400-44f4-8ecd-08af0d986719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 08:34:35 - INFO - Creating datasets...\n",
      "2025-01-10 08:34:35 - INFO - Created dataset with 186428 samples\n",
      "2025-01-10 08:34:35 - INFO - Number of labels: 7\n",
      "2025-01-10 08:34:37 - INFO - Average sequence length (words): 69.70\n",
      "2025-01-10 08:34:37 - INFO - Created dataset with 46607 samples\n",
      "2025-01-10 08:34:37 - INFO - Number of labels: 7\n",
      "2025-01-10 08:34:37 - INFO - Average sequence length (words): 69.59\n",
      "2025-01-10 08:34:37 - INFO - \n",
      "Creating data loaders with batch size 16...\n",
      "2025-01-10 08:34:37 - INFO - Number of training batches: 11652\n",
      "2025-01-10 08:34:37 - INFO - Number of validation batches: 2913\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "logging.info(\"Creating datasets...\")\n",
    "train_dataset = ShellAttackDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = ShellAttackDataset(X_val, y_val, tokenizer)\n",
    "    \n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "logging.info(f\"\\nCreating data loaders with batch size {batch_size}...\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "logging.info(f\"Number of training batches: {len(train_loader)}\")\n",
    "logging.info(f\"Number of validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4048a07e-558a-4a3d-a19c-70c5c0437229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 08:34:37 - INFO - \n",
      "Initializing model...\n",
      "2025-01-10 08:34:37 - INFO - Using device: cpu\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2025-01-10 08:34:43 - INFO - Initialized BERT Classifier with:\n",
      "2025-01-10 08:34:43 - INFO - - BERT hidden size: 768\n",
      "2025-01-10 08:34:43 - INFO - - Number of labels: 7\n",
      "2025-01-10 08:34:43 - INFO - - Dropout rate: 0.1\n",
      "2025-01-10 08:34:43 - INFO - Total parameters: 109,487,623\n",
      "2025-01-10 08:34:43 - INFO - Trainable parameters: 109,487,623\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "logging.info(\"\\nInitializing model...\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging.info(f\"Using device: {device}\")\n",
    "model = BertClassifier(num_labels=len(mlb.classes_))\n",
    "model.to(device)\n",
    "    \n",
    "# Count model parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logging.info(f\"Total parameters: {total_params:,}\")\n",
    "logging.info(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64600a25-6d2b-4bbb-8b7f-64a480d2e5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 08:34:43 - INFO - \n",
      "Starting model training...\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "2025-01-10 08:34:43 - INFO - Starting training with 116520 total steps\n",
      "Epoch 1/10:   0%|          | 1/11652 [01:32<300:29:17, 92.85s/it]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "logging.info(\"\\nStarting model training...\")\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader, device, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f3a38-8705-4bf5-9bf9-cc019080b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training:\n",
    "training_history = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'true_labels': y_true,\n",
    "    'predicted_probabilities': y_pred_probs,\n",
    "    'probabilities_per_epoch': y_pred_probs_per_epoch\n",
    "}\n",
    "\n",
    "save_training_results(model, training_history, mlb)\n",
    "\n",
    "# Later, to load:\n",
    "# model, history, class_names = load_training_results(BertClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee8c52-0126-4e3c-bc08-6380711dde0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc7075-92fb-4c85-bf76-a4ba7b3e41d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "logging.info(\"Plotting learning curves...\")\n",
    "plot_learning_curves(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159ddfdd-c5f3-4c62-9264-3061832d8517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log total execution time\n",
    "total_time = datetime.now() - start_time\n",
    "logging.info(f\"\\nTotal execution time: {total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d2f16f-4782-4f45-86e3-a1054390b798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c34f0-4b33-4514-acf5-26437b0a3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, f1_score\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c4b95e-3f29-4c8a-9a00-2250c2e7012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_over_epochs(y_true, y_pred_probs, epochs):\n",
    "    \"\"\"Plot TPR, FPR, TNR, FNR over epochs.\"\"\"\n",
    "    metrics = {\n",
    "        'TPR': [], 'FPR': [], 'TNR': [], 'FNR': []\n",
    "    }\n",
    "    \n",
    "    for epoch_preds in y_pred_probs:\n",
    "        epoch_pred_binary = (epoch_preds > 0.5).astype(int)\n",
    "        tp = np.sum((y_true == 1) & (epoch_pred_binary == 1), axis=0)\n",
    "        fp = np.sum((y_true == 0) & (epoch_pred_binary == 1), axis=0)\n",
    "        tn = np.sum((y_true == 0) & (epoch_pred_binary == 0), axis=0)\n",
    "        fn = np.sum((y_true == 1) & (epoch_pred_binary == 0), axis=0)\n",
    "        \n",
    "        metrics['TPR'].append(tp / (tp + fn))\n",
    "        metrics['FPR'].append(fp / (fp + tn))\n",
    "        metrics['TNR'].append(tn / (tn + fp))\n",
    "        metrics['FNR'].append(fn / (fn + tp))\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for metric, values in metrics.items():\n",
    "        plt.plot(range(1, epochs + 1), np.mean(values, axis=1), label=metric)\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Rate')\n",
    "    plt.title('Metrics Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_curves(train_losses, val_losses):\n",
    "    \"\"\"Plot training and validation loss curves.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curves(y_true, y_pred_probs, class_names):\n",
    "    \"\"\"Plot ROC curves for each class.\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(y_true.shape[1]):\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, i], y_pred_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'{class_names[i]} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for Each Class')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_pr_curves(y_true, y_pred_probs, class_names):\n",
    "    \"\"\"Plot Precision-Recall curves for each class.\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(y_true.shape[1]):\n",
    "        precision, recall, _ = precision_recall_curve(y_true[:, i], y_pred_probs[:, i])\n",
    "        plt.plot(recall, precision, label=class_names[i])\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curves for Each Class')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_prob_histograms(y_pred_probs, class_names):\n",
    "    \"\"\"Plot histograms of prediction probabilities for each class.\"\"\"\n",
    "    n_classes = len(class_names)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_classes + n_cols - 1) // n_cols\n",
    "    \n",
    "    plt.figure(figsize=(15, 5 * n_rows))\n",
    "    for i in range(n_classes):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        plt.hist(y_pred_probs[:, i], bins=50)\n",
    "        plt.title(f'Class: {class_names[i]}')\n",
    "        plt.xlabel('Prediction Probability')\n",
    "        plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_3d_roc(y_true, y_pred_probs, class_names, n_classes=3):\n",
    "    \"\"\"Plot 3D ROC curve for the first three classes.\"\"\"\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    for i in range(min(n_classes, len(class_names))):\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, i], y_pred_probs[:, i])\n",
    "        ax.plot(fpr, tpr, zs=i, zdir='y', label=class_names[i])\n",
    "    \n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('Class')\n",
    "    ax.set_zlabel('True Positive Rate')\n",
    "    ax.set_title('3D ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_f1_scores(y_true, y_pred_probs, class_names):\n",
    "    \"\"\"Plot class-wise F1 scores.\"\"\"\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "    f1_scores = [f1_score(y_true[:, i], y_pred[:, i]) for i in range(len(class_names))]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(class_names, f1_scores)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Class-wise F1 Scores')\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_performance_metrics(y_true, y_pred_probs, class_names):\n",
    "    \"\"\"Plot precision, recall, and F1 score for each class.\"\"\"\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "    metrics = {\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1': []\n",
    "    }\n",
    "    \n",
    "    for i in range(len(class_names)):\n",
    "        tp = np.sum((y_true[:, i] == 1) & (y_pred[:, i] == 1))\n",
    "        fp = np.sum((y_true[:, i] == 0) & (y_pred[:, i] == 1))\n",
    "        fn = np.sum((y_true[:, i] == 1) & (y_pred[:, i] == 0))\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        metrics['Precision'].append(precision)\n",
    "        metrics['Recall'].append(recall)\n",
    "        metrics['F1'].append(f1)\n",
    "    \n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    ax.bar(x - width, metrics['Precision'], width, label='Precision')\n",
    "    ax.bar(x, metrics['Recall'], width, label='Recall')\n",
    "    ax.bar(x + width, metrics['F1'], width, label='F1')\n",
    "    \n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Model Performance Metrics by Class')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f2874-99c7-4273-9a3c-9c4ea725033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the plotting functions\n",
    "from plotting_utils import *\n",
    "\n",
    "# Assuming you have these variables from your model training:\n",
    "# y_true: Ground truth labels\n",
    "# y_pred_probs: Model's predicted probabilities\n",
    "# train_losses: List of training losses per epoch\n",
    "# val_losses: List of validation losses per epoch\n",
    "# class_names: List of your label names\n",
    "# y_pred_probs_per_epoch: List of prediction probabilities for each epoch\n",
    "\n",
    "# Plot all metrics\n",
    "def plot_all_metrics(y_true, y_pred_probs, y_pred_probs_per_epoch, train_losses, val_losses, class_names):\n",
    "    # 1. Metrics over epochs\n",
    "    plot_metrics_over_epochs(y_true, y_pred_probs_per_epoch, len(train_losses))\n",
    "    \n",
    "    # 2. Loss curves\n",
    "    plot_loss_curves(train_losses, val_losses)\n",
    "    \n",
    "    # 3. ROC curves\n",
    "    plot_roc_curves(y_true, y_pred_probs, class_names)\n",
    "    \n",
    "    # 4. Precision-Recall curves\n",
    "    plot_pr_curves(y_true, y_pred_probs, class_names)\n",
    "    \n",
    "    # 5. Probability histograms\n",
    "    plot_prob_histograms(y_pred_probs, class_names)\n",
    "    \n",
    "    # 6. 3D ROC curve\n",
    "    plot_3d_roc(y_true, y_pred_probs, class_names)\n",
    "    \n",
    "    # 7. F1 scores\n",
    "    plot_f1_scores(y_true, y_pred_probs, class_names)\n",
    "    \n",
    "    # 8. Performance metrics\n",
    "    plot_performance_metrics(y_true, y_pred_probs, class_names)\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# After training your model:\n",
    "y_true = model_evaluation['true_labels']\n",
    "y_pred_probs = model_evaluation['predicted_probabilities']\n",
    "y_pred_probs_per_epoch = model_evaluation['probabilities_per_epoch']\n",
    "train_losses = model_evaluation['train_losses']\n",
    "val_losses = model_evaluation['val_losses']\n",
    "class_names = mlb.classes_  # from your MultiLabelBinarizer\n",
    "\n",
    "# Generate all plots\n",
    "plot_all_metrics(y_true, y_pred_probs, y_pred_probs_per_epoch, \n",
    "                train_losses, val_losses, class_names)\n",
    "\n",
    "# Or call individual functions as needed:\n",
    "plot_roc_curves(y_true, y_pred_probs, class_names)\n",
    "plot_loss_curves(train_losses, val_losses)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dc0532-27b7-4318-a144-33330ed9447f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
